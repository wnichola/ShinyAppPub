confusionMatrix(hat4, testing$diagnosis)$overall
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modFit1 <- train(diagnosis ~ .,data=training,method="rf")
modFit2 <- train(diagnosis ~ .,data=training,method="gbm")
modFit3 <- train(diagnosis ~ .,data=training,method="lda")
pred1 <- predict(modFit1, testing)
pred2 <- predict(modFit2, testing)
pred3 <- predict(modFit3, testing)
predDF <- data.frame(pred1, pred2, pred3, diagnosis=testing$diagnosis)
modFitC <- train(diagnosis ~ ., data=predDF,method="rf")
pred4 <- predict(modFitC, testing)
confusionMatrix(pred1, testing$diagnosis)$overall
confusionMatrix(pred2, testing$diagnosis)$overall
confusionMatrix(pred3, testing$diagnosis)$overall
confusionMatrix(pred4, testing$diagnosis)$overall
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData <- data.frame(diagnosis, predictors)
inTrain <- createDataPartition(adData$diagnosis, p=3/4)[[1]]
training <- adData[inTrain, ]
testing <- adData[-inTrain, ]
dim(adData) # 333 131
# head(adData)
set.seed(62433)
fitRf <- train(diagnosis ~ ., data=training, method="rf")
fitGBM <- train(diagnosis ~ ., data=training, method="gbm")
fitLDA <- train(diagnosis ~ ., data=training, method="lda")
predRf <- predict(fitRf, testing)
predGBM <- predict(fitGBM, testing)
predLDA <- predict(fitLDA, testing)
pred <- data.frame(predRf, predGBM, predLDA, diagnosis=testing$diagnosis)
# Stack the predictions together using random forests ("rf")
fit <- train(diagnosis ~., data=pred, method="rf")
predFit <- predict(fit, testing)
c1 <- confusionMatrix(predRf, testing$diagnosis)$overall[1]
c2 <- confusionMatrix(predGBM, testing$diagnosis)$overall[1]
c3 <- confusionMatrix(predLDA, testing$diagnosis)$overall[1]
c4 <- confusionMatrix(predFit, testing$diagnosis)$overall[1]
print(paste(c1, c2, c3, c4))
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modFit1 <- train(diagnosis ~ .,data=training,method="rf")
modFit2 <- train(diagnosis ~ .,data=training,method="gbm")
modFit3 <- train(diagnosis ~ .,data=training,method="lda")
pred1 <- predict(modFit1, testing)
pred2 <- predict(modFit2, testing)
pred3 <- predict(modFit3, testing)
predDF <- data.frame(pred1, pred2, pred3, diagnosis=testing$diagnosis)
modFitC <- train(diagnosis ~ ., data=predDF,method="rf")
pred4 <- predict(modFitC, testing)
c1 <- confusionMatrix(pred1, testing$diagnosis)$overall[1]
c2 <- confusionMatrix(pred2, testing$diagnosis)$overall[1]
c3 <- confusionMatrix(pred3, testing$diagnosis)$overall[1]
c4 <- confusionMatrix(pred4, testing$diagnosis)$overall[1]
print(paste(c1, c2, c3, c4))
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
M1 <- train(diagnosis ~ ., data=training, method="rf")
M2 <- train(diagnosis ~ ., data=training, method="gbm")
M3 <- train(diagnosis ~ ., data=training, method="lda")
hat1 <- predict(M1, testing)
hat2 <- predict(M2, testing)
hat3 <- predict(M3, testing)
hat <- data.frame(hat1, hat2, hat3, diagnosis=testing$diagnosis)
M4 <- train(diagnosis ~ ., data=hat, method="rf")
M4
hat4 <- predict(M4, testing)
c1 <- confusionMatrix(hat1, testing$diagnosis)$overall[1]
c2 <- confusionMatrix(hat2, testing$diagnosis)$overall[1]
c3 <- confusionMatrix(hat3, testing$diagnosis)$overall[1]
c4 <- confusionMatrix(hat4, testing$diagnosis)$overall[1]
print(paste(c1, c2, c3, c4))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
library(caret)
modFit_RF <- train(y ~ .,data=vowel.train,method="rf")
modFit_GBM <- train(y ~ .,data=vowel.train,method="gbm")
pred1 <- predict(modFit_RF, vowel.test)
pred2 <- predict(modFit_GBM, vowel.test)
predDF <- data.frame(pred1, pred2, y = vowel.test$y, agree=pred1==pred2)
rf <- confusionMatrix(pred1, vowel.test$y)$overall[1]
gbm <- confusionMatrix(pred2, vowel.test$y)$overall[1]
accuracy <- sum(pred1[predDF$agree] == predDF$y[predDF$agree]) / sum(predDF$agree)
print(paste(c(rf, gbm, accuracy)))
version
sessionInfo()
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
View(concrete)
?lasso
install.packages("lasso")
modFit <- train(CompressiveStrength ~ ., data=training, method="lasso")
?plot.enet
pred <- predict(modFit, testing)
plot(pred, xvar="penalty")
plot.enet(modFit$finalModel, xvar="penalty", use.color=T)
legend(modFit)
plot(modFit$finalModel, xvar="penalty")
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
?randomforest
?RandomForest
??randomforest
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modFit1 <- randomForest(diagnosis ~ .,data=training)
library(randomForest)
modFit1 <- randomForest(diagnosis ~ .,data=training)
modFit2 <- train(diagnosis ~ .,data=training,method="gbm")
modFit3 <- train(diagnosis ~ .,data=training,method="lda")
pred1 <- predict(modFit1, testing)
pred2 <- predict(modFit2, testing)
pred3 <- predict(modFit3, testing)
predDF <- data.frame(pred1, pred2, pred3, diagnosis=testing$diagnosis)
modFitC <- train(diagnosis ~ ., data=predDF,method="rf")
pred4 <- predict(modFitC, testing)
c1 <- confusionMatrix(pred1, testing$diagnosis)$overall[1]
c2 <- confusionMatrix(pred2, testing$diagnosis)$overall[1]
c3 <- confusionMatrix(pred3, testing$diagnosis)$overall[1]
c4 <- confusionMatrix(pred4, testing$diagnosis)$overall[1]
print(paste(c1, c2, c3, c4))
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
# Set the seed to 62433 and predict diagnosis with all the other variables using a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis ("lda") model. Stack the predictions together using random forests ("rf"). What is the resulting accuracy on the test set? Is it better or worse than each of the individual predictions?
# Stacked Accuracy: 0.80 is better than all three other methods
# Stacked Accuracy: 0.80 is better than random forests and lda and the same as boosting.
# Stacked Accuracy: 0.76 is better than lda but not random forests or boosting.
# Stacked Accuracy: 0.80 is worse than all the other methods.
# modFit1 <- train(diagnosis ~ .,data=training,method="rf")
library(randomForest)
modFit1 <- randomForest(diagnosis ~ .,data=training)
modFit2 <- train(diagnosis ~ .,data=training,method="gbm")
modFit3 <- train(diagnosis ~ .,data=training,method="lda")
# pred1 <- predict(modFit1, testing)
# pred2 <- predict(modFit2, testing)
# pred3 <- predict(modFit3, testing)
pred1 <- predict(modFit1, training)
pred2 <- predict(modFit2, training)
pred3 <- predict(modFit3, training)
predDF <- data.frame(pred1, pred2, pred3, diagnosis=testing$diagnosis)
modFitC <- train(diagnosis ~ ., data=predDF,method="rf")
pred4 <- predict(modFitC, training)
c1 <- confusionMatrix(pred1, testing$diagnosis)$overall[1]
c2 <- confusionMatrix(pred2, testing$diagnosis)$overall[1]
c1 <- confusionMatrix(pred1, training$diagnosis)$overall[1]
c2 <- confusionMatrix(pred2, training$diagnosis)$overall[1]
c3 <- confusionMatrix(pred3, training$diagnosis)$overall[1]
c4 <- confusionMatrix(pred4, training$diagnosis)$overall[1]
print(paste(c1, c2, c3, c4))
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
# Set the seed to 62433 and predict diagnosis with all the other variables using a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis ("lda") model. Stack the predictions together using random forests ("rf"). What is the resulting accuracy on the test set? Is it better or worse than each of the individual predictions?
# Stacked Accuracy: 0.80 is better than all three other methods
# Stacked Accuracy: 0.80 is better than random forests and lda and the same as boosting.
# Stacked Accuracy: 0.76 is better than lda but not random forests or boosting.
# Stacked Accuracy: 0.80 is worse than all the other methods.
# modFit1 <- train(diagnosis ~ .,data=training,method="rf")
library(randomForest)
modFit1 <- randomForest(diagnosis ~ .,data=training)
modFit2 <- train(diagnosis ~ .,data=training,method="gbm")
modFit3 <- train(diagnosis ~ .,data=training,method="lda")
# pred1 <- predict(modFit1, testing)
# pred2 <- predict(modFit2, testing)
# pred3 <- predict(modFit3, testing)
pred1 <- predict(modFit1, training)
pred2 <- predict(modFit2, training)
pred3 <- predict(modFit3, training)
# predDF <- data.frame(pred1, pred2, pred3, diagnosis=testing$diagnosis)
predDF <- data.frame(pred1, pred2, pred3, diagnosis=training$diagnosis)
modFitC <- train(diagnosis ~ ., data=predDF,method="rf")
# pred4 <- predict(modFitC, testing)
pred4 <- predict(modFitC, training)
c1 <- confusionMatrix(pred1, training$diagnosis)$overall[1]
c2 <- confusionMatrix(pred2, training$diagnosis)$overall[1]
c3 <- confusionMatrix(pred3, training$diagnosis)$overall[1]
c4 <- confusionMatrix(pred4, training$diagnosis)$overall[1]
# Load the concrete data with the commands:
print(paste(c(c1, c2, c3, c4)))
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
# Set the seed to 62433 and predict diagnosis with all the other variables using a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis ("lda") model. Stack the predictions together using random forests ("rf"). What is the resulting accuracy on the test set? Is it better or worse than each of the individual predictions?
# Stacked Accuracy: 0.80 is better than all three other methods
# Stacked Accuracy: 0.80 is better than random forests and lda and the same as boosting.
# Stacked Accuracy: 0.76 is better than lda but not random forests or boosting.
# Stacked Accuracy: 0.80 is worse than all the other methods.
# modFit1 <- train(diagnosis ~ .,data=training,method="rf")
library(randomForest)
modFit1 <- randomForest(diagnosis ~ .,data=training)
modFit2 <- train(diagnosis ~ .,data=training,method="gbm")
modFit3 <- train(diagnosis ~ .,data=training,method="lda")
pred1 <- predict(modFit1, testing)
pred2 <- predict(modFit2, testing)
pred3 <- predict(modFit3, testing)
# pred1 <- predict(modFit1, training)
# pred2 <- predict(modFit2, training)
# pred3 <- predict(modFit3, training)
predDF <- data.frame(pred1, pred2, pred3, diagnosis=testing$diagnosis)
# predDF <- data.frame(pred1, pred2, pred3, diagnosis=training$diagnosis)
modFitC <- train(diagnosis ~ ., data=predDF,method="rf")
pred4 <- predict(modFitC, testing)
# pred4 <- predict(modFitC, training)
c1 <- confusionMatrix(pred1, testing$diagnosis)$overall[1]
c2 <- confusionMatrix(pred2, testing$diagnosis)$overall[1]
c3 <- confusionMatrix(pred3, testing$diagnosis)$overall[1]
c4 <- confusionMatrix(pred4, testing$diagnosis)$overall[1]
# c1 <- confusionMatrix(pred1, training$diagnosis)$overall[1]
# c2 <- confusionMatrix(pred2, training$diagnosis)$overall[1]
# c3 <- confusionMatrix(pred3, training$diagnosis)$overall[1]
# c4 <- confusionMatrix(pred4, training$diagnosis)$overall[1]
print(paste(c(c1, c2, c3, c4)))
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
# Set the seed to 62433 and predict diagnosis with all the other variables using a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis ("lda") model. Stack the predictions together using random forests ("rf"). What is the resulting accuracy on the test set? Is it better or worse than each of the individual predictions?
# Stacked Accuracy: 0.80 is better than all three other methods
# Stacked Accuracy: 0.80 is better than random forests and lda and the same as boosting.
# Stacked Accuracy: 0.76 is better than lda but not random forests or boosting.
# Stacked Accuracy: 0.80 is worse than all the other methods.
modFit1 <- train(diagnosis ~ .,data=training,method="rf")
# library(randomForest)
# modFit1 <- randomForest(diagnosis ~ .,data=training)
modFit2 <- train(diagnosis ~ .,data=training,method="gbm")
modFit3 <- train(diagnosis ~ .,data=training,method="lda")
pred1 <- predict(modFit1, testing)
pred2 <- predict(modFit2, testing)
pred3 <- predict(modFit3, testing)
# pred1 <- predict(modFit1, training)
# pred2 <- predict(modFit2, training)
# pred3 <- predict(modFit3, training)
predDF <- data.frame(pred1, pred2, pred3, diagnosis=testing$diagnosis)
# predDF <- data.frame(pred1, pred2, pred3, diagnosis=training$diagnosis)
modFitC <- train(diagnosis ~ ., data=predDF,method="rf")
pred4 <- predict(modFitC, testing)
# pred4 <- predict(modFitC, training)
c1 <- confusionMatrix(pred1, testing$diagnosis)$overall[1]
c2 <- confusionMatrix(pred2, testing$diagnosis)$overall[1]
c3 <- confusionMatrix(pred3, testing$diagnosis)$overall[1]
c4 <- confusionMatrix(pred4, testing$diagnosis)$overall[1]
# c1 <- confusionMatrix(pred1, training$diagnosis)$overall[1]
# c2 <- confusionMatrix(pred2, training$diagnosis)$overall[1]
# c3 <- confusionMatrix(pred3, training$diagnosis)$overall[1]
# c4 <- confusionMatrix(pred4, training$diagnosis)$overall[1]
print(paste(c(c1, c2, c3, c4)))
Sys.info()
sessionInfo()
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# set.seed(62433)
# Set the seed to 62433 and predict diagnosis with all the other variables using a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis ("lda") model. Stack the predictions together using random forests ("rf"). What is the resulting accuracy on the test set? Is it better or worse than each of the individual predictions?
# Stacked Accuracy: 0.80 is better than all three other methods
# Stacked Accuracy: 0.80 is better than random forests and lda and the same as boosting.
# Stacked Accuracy: 0.76 is better than lda but not random forests or boosting.
# Stacked Accuracy: 0.80 is worse than all the other methods.
modFit1 <- train(diagnosis ~ .,data=training,method="rf")
# library(randomForest)
# modFit1 <- randomForest(diagnosis ~ .,data=training)
modFit2 <- train(diagnosis ~ .,data=training,method="gbm")
modFit3 <- train(diagnosis ~ .,data=training,method="lda")
pred1 <- predict(modFit1, testing)
pred2 <- predict(modFit2, testing)
pred3 <- predict(modFit3, testing)
# pred1 <- predict(modFit1, training)
# pred2 <- predict(modFit2, training)
# pred3 <- predict(modFit3, training)
predDF <- data.frame(pred1, pred2, pred3, diagnosis=testing$diagnosis)
# predDF <- data.frame(pred1, pred2, pred3, diagnosis=training$diagnosis)
modFitC <- train(diagnosis ~ ., data=predDF,method="rf")
pred4 <- predict(modFitC, testing)
# pred4 <- predict(modFitC, training)
c1 <- confusionMatrix(pred1, testing$diagnosis)$overall[1]
c2 <- confusionMatrix(pred2, testing$diagnosis)$overall[1]
c3 <- confusionMatrix(pred3, testing$diagnosis)$overall[1]
c4 <- confusionMatrix(pred4, testing$diagnosis)$overall[1]
# c1 <- confusionMatrix(pred1, training$diagnosis)$overall[1]
# c2 <- confusionMatrix(pred2, training$diagnosis)$overall[1]
# c3 <- confusionMatrix(pred3, training$diagnosis)$overall[1]
# c4 <- confusionMatrix(pred4, training$diagnosis)$overall[1]
print(paste(c(c1, c2, c3, c4)))
# Question 2
# Load the Alzheimer's data using the following commands
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
# Set the seed to 62433 and predict diagnosis with all the other variables using a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis ("lda") model. Stack the predictions together using random forests ("rf"). What is the resulting accuracy on the test set? Is it better or worse than each of the individual predictions?
# Stacked Accuracy: 0.80 is better than all three other methods
# Stacked Accuracy: 0.80 is better than random forests and lda and the same as boosting.
# Stacked Accuracy: 0.76 is better than lda but not random forests or boosting.
# Stacked Accuracy: 0.80 is worse than all the other methods.
modFit1 <- train(diagnosis ~ .,data=training,method="rf")
modFit2 <- train(diagnosis ~ .,data=training,method="gbm")
modFit3 <- train(diagnosis ~ .,data=training,method="lda")
pred1 <- predict(modFit1, testing)
pred2 <- predict(modFit2, testing)
pred3 <- predict(modFit3, testing)
# pred1 <- predict(modFit1, training)
# pred2 <- predict(modFit2, training)
# pred3 <- predict(modFit3, training)
predDF <- data.frame(pred1, pred2, pred3, diagnosis=testing$diagnosis)
predDFT <- data.frame(pred1, pred2, pred3, diagnosis=training$diagnosis)
# predDF <- data.frame(pred1, pred2, pred3, diagnosis=training$diagnosis)
modFitC <- train(diagnosis ~ ., data=predDF,method="rf")
modFitCT <- train(diagnosis ~ ., data=predDFT,method="rf")
predDFT <- data.frame(pred1, pred2, pred3, diagnosis=training$diagnosis)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modFit1 <- train(diagnosis ~ .,data=training,method="rf")
# library(randomForest)
# modFit1 <- randomForest(diagnosis ~ .,data=training)
modFit2 <- train(diagnosis ~ .,data=training,method="gbm")
modFit3 <- train(diagnosis ~ .,data=training,method="lda")
pred1 <- predict(modFit1, testing)
pred2 <- predict(modFit2, testing)
pred3 <- predict(modFit3, testing)
# pred1 <- predict(modFit1, training)
# pred2 <- predict(modFit2, training)
# pred3 <- predict(modFit3, training)
predDF <- data.frame(pred1, pred2, pred3, diagnosis=testing$diagnosis)
predDFT <- data.frame(pred1, pred2, pred3, diagnosis=training$diagnosis)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modFit1 <- train(diagnosis ~ .,data=training,method="rf")
modFit2 <- train(diagnosis ~ .,data=training,method="gbm")
modFit3 <- train(diagnosis ~ .,data=training,method="lda")
pred1 <- predict(modFit1, testing)
pred2 <- predict(modFit2, testing)
pred3 <- predict(modFit3, testing)
predDF <- data.frame(pred1, pred2, pred3, diagnosis=testing$diagnosis)
modFitC <- train(diagnosis ~ ., data=predDF,method="rf")
pred4 <- predict(modFitC, testing)
c1 <- confusionMatrix(pred1, testing$diagnosis)$overall[1]
c2 <- confusionMatrix(pred2, testing$diagnosis)$overall[1]
c3 <- confusionMatrix(pred3, testing$diagnosis)$overall[1]
c4 <- confusionMatrix(pred4, testing$diagnosis)$overall[1]
print(paste(c(c1, c2, c3, c4)))
install.packages("devtools")
library(rCharts)
install.packages("rCharts")
data("airquality")
dTable(airquality, sPaginationType = "full_numbers")
library(dTable)
?dTable
??dTable
d <- data.frame(airquality, stringsAsFactors = FALSE) print(d)
d <- data.frame(airquality, stringsAsFactors = FALSE); print(d)
airquality
# head(airquality)
class(d)
class(airquality)
library(rCharts)
library(rChart)
library(rCharts)
install.packages("rCharts")
library(rCharts)
require(rCharts)
require(devtools)
install_github('rCharts', 'ramnathv')
require(devtools)
shinyapps::setAccountInfo(name='wnichola', token='EF0B05472481422A6CB620D1C7B91095', secret='iUf19Nt6HTWqrVNdUlYnuMYB+iOEqplnSZg9jFst')
shiny::runApp('09 Developing Data Products/Project')
shiny::runApp('09 Developing Data Products/Project')
shiny::runApp('09 Developing Data Products/Project')
shiny::runApp('09 Developing Data Products/Project')
shiny::runApp('09 Developing Data Products/Project')
shiny::runApp('09 Developing Data Products/Project')
shiny::runApp('09 Developing Data Products/Project')
shiny::runApp('09 Developing Data Products/Project')
shiny::runApp('09 Developing Data Products/Project')
pie.sum = data.frame(CATEGORY = c('Cat1','Cat2','Cat3','Cat4'),
VALUE = c(124,55,275,20))
pie.sum$PERCENT = round((pie.sum$VALUE/sum(pie.sum$VALUE)) * 100,2)
library(rCharts)
n3 = nPlot(x = "CATEGORY", y = "VALUE", data = pie.sum, type = "pieChart")
n3$chart(tooltipContent = "#! function(key, y, e, graph){return '<h3>' + 'Category: ' + key + '</h3>' + '<p>'+ 'Value ' + y + '<br>' + ' % of value: ' + e.point.PERCENT} !#")
n3$set(width = 800, height = 500) # mk changed width to 800 and height to 500
n3
setwd("C:/Data/My Project/MOCC/03 Quiz and Projects/09 Developing Data Products/SlidifyProject")
publish(title = 'ShinyApps Presentation', 'index.html', host = 'rpubs')
library(slidify)
publish(title = 'ShinyApps Presentation', 'index.html', host = 'rpubs')
publish(title = 'ShinyApps Presentation', 'index.html', host = 'rpubs')
find.package("RCurl")
.libPaths( "")
publish(title = 'ShinyApps Presentation', 'index.html', host = 'rpubs')
find.package("RCurl")
.libPaths( "C:/Users/Nicholas/Documents/R/win-library/3.2/RCurl")
publish(title = 'ShinyApps Presentation', 'index.html', host = 'rpubs')
publish(user="wnichola", repo="ShinyAppPub")
publish(user="wnichola", repo="ShinyAppPub")
publish(user="wnichola", repo="ShinyAppPub")
